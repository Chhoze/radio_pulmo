{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKeVGxZ5GG6o"
   },
   "source": [
    "# covid-19-radiology-vgg19-f1-score-95\n",
    "\n",
    "https://www.kaggle.com/code/ahmedtronic/covid-19-radiology-vgg19-f1-score-95/notebook\n",
    "\n",
    "# Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:32.951453Z",
     "iopub.status.busy": "2023-03-05T21:59:32.951159Z",
     "iopub.status.idle": "2023-03-05T21:59:42.881158Z",
     "shell.execute_reply": "2023-03-05T21:59:42.879899Z",
     "shell.execute_reply.started": "2023-03-05T21:59:32.951421Z"
    },
    "id": "CeMcAy_5GG6s",
    "outputId": "8e007371-6c2c-492c-99bb-172286922ae2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import system libs\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "# import data handling tools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# import Deep learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print ('modules loaded')\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA_gwvwnGG6v"
   },
   "source": [
    "# Create needed functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4reLHLHabWD"
   },
   "source": [
    "## Functions to Create Data Frame from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQdhl_CRGG6v"
   },
   "source": [
    "#### **Function to create data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.884292Z",
     "iopub.status.busy": "2023-03-05T21:59:42.883302Z",
     "iopub.status.idle": "2023-03-05T21:59:42.896918Z",
     "shell.execute_reply": "2023-03-05T21:59:42.895696Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.884252Z"
    },
    "id": "g2nDmYaAabWE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate data paths with labels\n",
    "def define_paths(data_dir):\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "\n",
    "    folds = os.listdir(data_dir)\n",
    "    for fold in folds:\n",
    "        foldpath = os.path.join(data_dir, fold)\n",
    "        # check the folders from main directory. If there are another files, ignore them\n",
    "        if pathlib.Path(foldpath).suffix != '':\n",
    "            continue\n",
    "\n",
    "        filelist = os.listdir(foldpath)\n",
    "        for file in filelist:\n",
    "            fpath = os.path.join(foldpath, file)\n",
    "\n",
    "            # check if there are another folders\n",
    "            if pathlib.Path(foldpath).suffix == '':\n",
    "                # check unneeded masks\n",
    "                if pathlib.Path(fpath).parts[-1] == 'masks' or pathlib.Path(fpath).parts[-1] == 'Masks' or pathlib.Path(fpath).parts[-1] == 'MASKS':\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    o_file = os.listdir(fpath)\n",
    "                    for f in o_file:\n",
    "                        ipath = os.path.join(fpath, f)\n",
    "                        filepaths.append(ipath)\n",
    "                        labels.append(fold)\n",
    "\n",
    "            else:\n",
    "                filepaths.append(fpath)\n",
    "                labels.append(fold)\n",
    "\n",
    "    return filepaths, labels\n",
    "\n",
    "\n",
    "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
    "def define_df(files, classes):\n",
    "    Fseries = pd.Series(files, name= 'filepaths')\n",
    "    Lseries = pd.Series(classes, name='labels')\n",
    "    return pd.concat([Fseries, Lseries], axis= 1)\n",
    "\n",
    "# Split dataframe to train, valid, and test\n",
    "def split_data(data_dir):\n",
    "    # train dataframe\n",
    "    files, classes = define_paths(data_dir)\n",
    "    df = define_df(files, classes)\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    # valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
    "\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZaHdeFxGG6x"
   },
   "source": [
    "#### Function to generate images from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.899100Z",
     "iopub.status.busy": "2023-03-05T21:59:42.898621Z",
     "iopub.status.idle": "2023-03-05T21:59:42.916158Z",
     "shell.execute_reply": "2023-03-05T21:59:42.915157Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.899002Z"
    },
    "id": "iLL8hHQcGG6x",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_gens (train_df, valid_df, test_df, batch_size):\n",
    "    '''\n",
    "    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
    "    Image data generator converts images into tensors. '''\n",
    "\n",
    "\n",
    "    # define model parameters\n",
    "    img_size = (224, 224)\n",
    "    channels = 3 # either BGR or Grayscale\n",
    "    color = 'rgb'\n",
    "    img_shape = (img_size[0], img_size[1], channels)\n",
    "\n",
    "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "    test_steps = ts_length // test_batch_size\n",
    "\n",
    "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "    def scalar(img):\n",
    "        return img\n",
    "\n",
    "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True,\n",
    "                                width_shift_range=0.1, \n",
    "                                height_shift_range=0.1,\n",
    "                                rotation_range=5, fill_mode='nearest',\n",
    "                                brightness_range=[0.8,1.2],\n",
    "                                )\n",
    "    ts_gen = ImageDataGenerator(preprocessing_function= scalar,\n",
    "                                width_shift_range=0.1, \n",
    "                                height_shift_range=0.1,\n",
    "                                rotation_range=5, fill_mode='nearest',\n",
    "                                brightness_range=[0.8,1.2],\n",
    "                                )\n",
    "\n",
    "    train_gen = tr_gen.flow_from_dataframe(train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    valid_gen = ts_gen.flow_from_dataframe(valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
    "\n",
    "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
    "    test_gen = ts_gen.flow_from_dataframe(test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
    "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "\n",
    "    return train_gen, valid_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ifXox4SGG6y"
   },
   "source": [
    "#### **Function to display data sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.919918Z",
     "iopub.status.busy": "2023-03-05T21:59:42.919554Z",
     "iopub.status.idle": "2023-03-05T21:59:42.930338Z",
     "shell.execute_reply": "2023-03-05T21:59:42.929392Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.919882Z"
    },
    "id": "IAGbj3ZyGG6y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_images(gen):\n",
    "    '''\n",
    "    This function take the data generator and show sample of the images\n",
    "    '''\n",
    "\n",
    "    # return classes , images to be displayed\n",
    "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
    "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
    "    images, labels = next(gen)        # get a batch size samples from the generator\n",
    "\n",
    "    # calculate number of displayed samples\n",
    "    length = len(labels)        # length of batch size\n",
    "    sample = min(length, 25)    # check if sample less than 25 images\n",
    "\n",
    "    plt.figure(figsize= (20, 20))\n",
    "\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255       # scales data to range (0 - 255)\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])  # get image index\n",
    "        class_name = classes[index]   # get class of image\n",
    "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K-ryg0DGG6z"
   },
   "source": [
    "#### **Callbacks** \n",
    "<br> \n",
    "Callbacks : Helpful functions to help optimize model training  <br> \n",
    "Examples: stop model training after specfic time, stop training if no improve in accuracy and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.932410Z",
     "iopub.status.busy": "2023-03-05T21:59:42.932034Z",
     "iopub.status.idle": "2023-03-05T21:59:42.964243Z",
     "shell.execute_reply": "2023-03-05T21:59:42.963294Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.932374Z"
    },
    "id": "d5HiN8XDGG60",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Without lr setting for cos and exp decay\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
    "        super(MyCallback, self).__init__()\n",
    "        #self.model = model\n",
    "        self.my_model = model\n",
    "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor = factor # factor by which to reduce the learning rate\n",
    "        self.batches = batches # number of training batch to run per epoch\n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
    "\n",
    "        # callback variables\n",
    "        self.count = 0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   # epoch with the lowest loss\n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate)) # get the initial learning rate and save it\n",
    "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights = self.my_model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights = self.my_model.get_weights()   # save initial weights if they have to get restored\n",
    "\n",
    "    # Define a function that will run when train begins\n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = input('')\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "\n",
    "        # set the weights of the model to the best weights\n",
    "        self.my_model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "    # Define method runs on the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.my_model.optimizer.learning_rate)) # get the current learning rate\n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        loss = logs.get('loss')  # get training loss for this epoch\n",
    "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
    "\n",
    "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
    "                self.highest_tracc = acc # set new highest training accuracy\n",
    "                self.best_weights = self.my_model.get_weights() # training accuracy improved so save the weights\n",
    "                self.count = 0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count = 0 # set stop counter to 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
    "\n",
    "            else:\n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count >= self.patience - 1: # lr should be adjusted\n",
    "                    lr = lr * self.factor # adjust the learning by factor\n",
    "                    #tf.keras.backend.set_value(self.my_model.optimizer.learning_rate, lr) # set the learning rate in the optimizer\n",
    "                    #self.my_model.optimizer.learning_rate = lr # set the learning rate in the optimizer\n",
    "                    self.count = 0 # reset the count to 0\n",
    "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count = 0 # reset counter\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment patience counter\n",
    "\n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
    "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
    "                self.best_weights = self.my_model.get_weights() # validation loss improved so save the weights\n",
    "                self.count = 0 # reset count since validation loss improved\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
    "\n",
    "            else: # validation loss did not improve\n",
    "                if self.count >= self.patience - 1: # need to adjust lr\n",
    "                    lr = lr * self.factor # adjust the learning rate\n",
    "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
    "                    self.count = 0 # reset counter\n",
    "                    #tf.keras.backend.set_value(self.my_model.optimizer.learning_rate, lr) # set the learning rate in the optimizer\n",
    "                    #self.my_model.optimizer.learning_rate = lr # set the learning rate in the optimizer\n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment the patience counter\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.my_model.stop_training = True # stop training\n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.my_model.stop_training = True # stop training\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With lr setting (verison use for main model training)\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
    "        super(MyCallback, self).__init__()\n",
    "        #self.model = model\n",
    "        self.my_model = model\n",
    "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor = factor # factor by which to reduce the learning rate\n",
    "        self.batches = batches # number of training batch to run per epoch\n",
    "        self.epochs = epochs\n",
    "        self.ask_epoch = ask_epoch\n",
    "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
    "\n",
    "        # callback variables\n",
    "        self.count = 0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   # epoch with the lowest loss\n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.learning_rate)) # get the initial learning rate and save it\n",
    "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights = self.my_model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights = self.my_model.get_weights()   # save initial weights if they have to get restored\n",
    "\n",
    "    # Define a function that will run when train begins\n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = input('')\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "\n",
    "        # set the weights of the model to the best weights\n",
    "        self.my_model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "    # Define method runs on the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.my_model.optimizer.learning_rate)) # get the current learning rate\n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        loss = logs.get('loss')  # get training loss for this epoch\n",
    "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
    "\n",
    "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
    "                self.highest_tracc = acc # set new highest training accuracy\n",
    "                self.best_weights = self.my_model.get_weights() # training accuracy improved so save the weights\n",
    "                self.count = 0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count = 0 # set stop counter to 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
    "\n",
    "            else:\n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count >= self.patience - 1: # lr should be adjusted\n",
    "                    lr = lr * self.factor # adjust the learning by factor\n",
    "                    #tf.keras.backend.set_value(self.my_model.optimizer.learning_rate, lr) # set the learning rate in the optimizer\n",
    "                    self.my_model.optimizer.learning_rate = lr # set the learning rate in the optimizer\n",
    "                    self.count = 0 # reset the count to 0\n",
    "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count = 0 # reset counter\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment patience counter\n",
    "\n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
    "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
    "                self.best_weights = self.my_model.get_weights() # validation loss improved so save the weights\n",
    "                self.count = 0 # reset count since validation loss improved\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
    "\n",
    "            else: # validation loss did not improve\n",
    "                if self.count >= self.patience - 1: # need to adjust lr\n",
    "                    lr = lr * self.factor # adjust the learning rate\n",
    "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
    "                    self.count = 0 # reset counter\n",
    "                    #tf.keras.backend.set_value(self.my_model.optimizer.learning_rate, lr) # set the learning rate in the optimizer\n",
    "                    self.my_model.optimizer.learning_rate = lr # set the learning rate in the optimizer\n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment the patience counter\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.my_model.stop_training = True # stop training\n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.my_model.stop_training = True # stop training\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zwhoj3zGG61"
   },
   "source": [
    "#### **Function to plot history of training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.966593Z",
     "iopub.status.busy": "2023-03-05T21:59:42.965705Z",
     "iopub.status.idle": "2023-03-05T21:59:42.977910Z",
     "shell.execute_reply": "2023-03-05T21:59:42.976940Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.966551Z"
    },
    "id": "pU3eAW5jGG62",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_training(hist, plt_fig_size=(20, 8)):\n",
    "    '''\n",
    "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
    "    '''\n",
    "\n",
    "    # Define needed variables\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize= plt_fig_size)\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.plot(Epochs, tr_loss, 'orange', label= 'Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'deepskyblue', label= 'Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'orange', label= 'Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'deepskyblue', label= 'Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK6cgu7LGG63"
   },
   "source": [
    "#### **Function to create Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.980527Z",
     "iopub.status.busy": "2023-03-05T21:59:42.979559Z",
     "iopub.status.idle": "2023-03-05T21:59:42.993257Z",
     "shell.execute_reply": "2023-03-05T21:59:42.992265Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.980488Z"
    },
    "id": "_4mPYHnzGG64",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues, plt_fig_size=(10, 10)):\n",
    "\t'''\n",
    "\tThis function plot confusion matrix method from sklearn package.\n",
    "\t'''\n",
    "\n",
    "\tplt.figure(figsize= plt_fig_size)\n",
    "\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
    "\tplt.title(title)\n",
    "\tplt.colorbar()\n",
    "\tplt.grid(visible=None)\n",
    "\n",
    "\ttick_marks = np.arange(len(classes))\n",
    "\tplt.xticks(tick_marks, classes, rotation= 45)\n",
    "\tplt.yticks(tick_marks, classes)\n",
    "\n",
    "\tif normalize:\n",
    "\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
    "\t\tprint('Normalized Confusion Matrix')\n",
    "\n",
    "\telse:\n",
    "\t\tprint('Confusion Matrix, Without Normalization')\n",
    "\n",
    "\tprint(cm)\n",
    "\n",
    "\tthresh = cm.max() / 2.\n",
    "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.ylabel('True Label')\n",
    "\tplt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57eDFl3oGG65"
   },
   "source": [
    "# **Model Structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GHNMVrhGG65"
   },
   "source": [
    "#### **Start Reading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-05T21:59:42.995566Z",
     "iopub.status.busy": "2023-03-05T21:59:42.994672Z",
     "iopub.status.idle": "2023-03-05T22:00:37.343149Z",
     "shell.execute_reply": "2023-03-05T22:00:37.341972Z",
     "shell.execute_reply.started": "2023-03-05T21:59:42.995528Z"
    },
    "id": "FWfxfQEVabWS",
    "outputId": "d8be6a8d-5b19-49f7-bfa1-09a96ff58286",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset'\n",
    "data_dir = '/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/raw/COVID-19_Radiography_Dataset'\n",
    "data_dir = '/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy'\n",
    "\n",
    "try:\n",
    "    # Get splitted data\n",
    "    train_df, valid_df, test_df = split_data(data_dir)\n",
    "\n",
    "    # Get Generators\n",
    "    batch_size = 16\n",
    "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
    "\n",
    "except:\n",
    "    print('Invalid Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Display Image Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:00:37.345595Z",
     "iopub.status.busy": "2023-03-05T22:00:37.344694Z",
     "iopub.status.idle": "2023-03-05T22:00:39.524855Z",
     "shell.execute_reply": "2023-03-05T22:00:39.523470Z",
     "shell.execute_reply.started": "2023-03-05T22:00:37.345556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_images(train_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wvOKjeRGG65"
   },
   "source": [
    "#### **Generic Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Structure\n",
    "img_size = (224, 224)\n",
    "channels = 3\n",
    "img_shape = (img_size[0], img_size[1], channels)\n",
    "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model VGG19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:00:39.528818Z",
     "iopub.status.busy": "2023-03-05T22:00:39.528237Z",
     "iopub.status.idle": "2023-03-05T22:00:43.435902Z",
     "shell.execute_reply": "2023-03-05T22:00:43.434974Z",
     "shell.execute_reply.started": "2023-03-05T22:00:39.528717Z"
    },
    "id": "kDT4CV15abWT",
    "outputId": "365637a8-7535-4ac4-90ea-700f6eb5769e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.vgg19.VGG19(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model EfficientNetB0**\n",
    "\n",
    "Taken from kaggle: [covid-19-classification-efficientnetb0-96-32](https://www.kaggle.com/code/abdallahwagih/covid-19-classification-efficientnetb0-96-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "    Dense(256, kernel_regularizer= regularizers.l2(l2= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "    Dropout(rate= 0.45, seed= 123),\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model EfficientNetB4**\n",
    "\n",
    "Adapted from kaggle: [covid-19-classification-efficientnetb0-96-32](https://www.kaggle.com/code/abdallahwagih/covid-19-classification-efficientnetb0-96-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.efficientnet.EfficientNetB4(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
    "    Dense(256, kernel_regularizer= regularizers.l2(l2= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
    "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
    "    Dropout(rate= 0.45, seed= 123),\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model DenseNet169**\n",
    "\n",
    "Taken from kaggle: [covid-19-radiology-densenet169](https://www.kaggle.com/code/ahmedtronic/covid-19-radiology-densenet169-f1-score-96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.densenet.DenseNet169(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model ResNet101**\n",
    "\n",
    "Taken from kaggle: [covid-19-radiology-ResNet101](https://www.kaggle.com/code/ahmedtronic/covid-19-radiology-resnet101-f1-score-94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.resnet.ResNet101(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
    "# we will use efficientnetb3 from EfficientNet family.\n",
    "base_model = tf.keras.applications.resnet.ResNet50(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dense(class_count, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer Cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "#decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch)\n",
    "decay_steps = 40 * 1059\n",
    "cosine_decay = CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay\n",
    "optimizer = Adam(learning_rate=cosine_decay)\n",
    "\n",
    "model.compile(optimizer, loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# TypeError: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. \n",
    "# If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optimizer Exponential**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001\n",
    "#decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch)\n",
    "decay_steps = 40 * 1059\n",
    "decay_rate = 0.96\n",
    "#exp_decay = ExponentialDecay(initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer, loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_gen) // 32 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TciwhdM1GG66"
   },
   "source": [
    "#### **Set Callback Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:00:43.437746Z",
     "iopub.status.busy": "2023-03-05T22:00:43.437375Z",
     "iopub.status.idle": "2023-03-05T22:00:43.606961Z",
     "shell.execute_reply": "2023-03-05T22:00:43.605816Z",
     "shell.execute_reply.started": "2023-03-05T22:00:43.437706Z"
    },
    "id": "7abvdv7mGG66",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16   # set batch size for training\n",
    "epochs = 100   # number of all epochs in training\n",
    "patience = 3   #number of epochs to wait to adjust lr if monitored value does not improve\n",
    "stop_patience = 10   # number of epochs to wait before stopping training if monitored value does not improve\n",
    "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
    "factor = 0.5   # factor to reduce lr by\n",
    "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
    "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
    "\n",
    "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
    "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(tf.keras.backend.count_params(layer) for layer in model.trainable_weights)\n",
    "non_trainable_params = sum(tf.keras.backend.count_params(layer) for layer in model.non_trainable_weights)\n",
    "\n",
    "print(trainable_params)\n",
    "print(non_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ap89fjdxGG67"
   },
   "source": [
    "#### **Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:00:43.608964Z",
     "iopub.status.busy": "2023-03-05T22:00:43.608596Z",
     "iopub.status.idle": "2023-03-05T22:41:21.836963Z",
     "shell.execute_reply": "2023-03-05T22:41:21.835819Z",
     "shell.execute_reply.started": "2023-03-05T22:00:43.608925Z"
    },
    "id": "0Uk3BTERGG67",
    "outputId": "ec610f68-a1a5-4c7d-9969-26dfab2d0305",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
    "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNKq6ebOGG67"
   },
   "source": [
    "#### **Display model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:41:21.838667Z",
     "iopub.status.busy": "2023-03-05T22:41:21.838298Z",
     "iopub.status.idle": "2023-03-05T22:41:22.365021Z",
     "shell.execute_reply": "2023-03-05T22:41:22.364129Z",
     "shell.execute_reply.started": "2023-03-05T22:41:21.838628Z"
    },
    "id": "L0Bj0Sp_GG68",
    "outputId": "663963ec-ea21-4272-8dda-a16c5f5e2ce5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MySXhfAJGG68"
   },
   "source": [
    "# **Evaluate model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:41:22.367517Z",
     "iopub.status.busy": "2023-03-05T22:41:22.366440Z",
     "iopub.status.idle": "2023-03-05T22:42:09.492412Z",
     "shell.execute_reply": "2023-03-05T22:42:09.491197Z",
     "shell.execute_reply.started": "2023-03-05T22:41:22.367474Z"
    },
    "id": "wSKDkyXXGG68",
    "outputId": "b521980b-a33b-421b-8cdf-4d92fb0f304a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ts_length = len(test_df)\n",
    "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
    "test_steps = ts_length // test_batch_size\n",
    "\n",
    "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
    "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
    "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
    "\n",
    "print(\"Train Loss: \", train_score[0])\n",
    "print(\"Train Accuracy: \", train_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Validation Loss: \", valid_score[0])\n",
    "print(\"Validation Accuracy: \", valid_score[1])\n",
    "print('-' * 20)\n",
    "print(\"Test Loss: \", test_score[0])\n",
    "print(\"Test Accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l-DABtFGG68"
   },
   "source": [
    "# **Get Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:42:09.494594Z",
     "iopub.status.busy": "2023-03-05T22:42:09.494195Z",
     "iopub.status.idle": "2023-03-05T22:42:19.004820Z",
     "shell.execute_reply": "2023-03-05T22:42:19.003801Z",
     "shell.execute_reply.started": "2023-03-05T22:42:09.494553Z"
    },
    "id": "GDFj7MZdGG69",
    "outputId": "6dbce8ed-fc8c-4398-b8bd-1ce8cb403727",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_gen)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJscUTF6GG69"
   },
   "source": [
    "#### **Confusion Matrics and Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:42:19.006686Z",
     "iopub.status.busy": "2023-03-05T22:42:19.006333Z",
     "iopub.status.idle": "2023-03-05T22:42:19.640843Z",
     "shell.execute_reply": "2023-03-05T22:42:19.639916Z",
     "shell.execute_reply.started": "2023-03-05T22:42:19.006650Z"
    },
    "id": "tQR-UlD6GG69",
    "outputId": "09ac1d97-2053-4633-e066-ca11540a2e27",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "g_dict = test_gen.class_indices\n",
    "classes = list(g_dict.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_gen.classes, y_pred)\n",
    "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsIK5v0lGG69"
   },
   "source": [
    "#### **Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Resnet50'# model.input_names[0][:-6]\n",
    "subject = 'Covid-19'\n",
    "acc = test_score[1] * 100\n",
    "save_path = f'/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/{model_name}'\n",
    "\n",
    "# Save model\n",
    "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.keras')\n",
    "model_save_loc = os.path.join(save_path, save_id)\n",
    "model.save(model_save_loc)\n",
    "print(f'model was saved as {model_save_loc}')\n",
    "\n",
    "# Save weights\n",
    "weight_save_id = str(f'{model_name}-{subject}.weights.h5')\n",
    "weights_save_loc = os.path.join(save_path, weight_save_id)\n",
    "model.save_weights(weights_save_loc)\n",
    "print(f'weights were saved as {weights_save_loc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_folder = 'Resnet101'\n",
    "#save_id = \"Resnet101-Covid-19-94.19.keras\"\n",
    "#save_path = f'/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/{model_folder}/{save_id}'\n",
    "\n",
    "save = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/EfficientNetB4_masked/EfficientNetB4_masked-Covid-19_masked-91.45.keras\"\n",
    "#weight = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/EfficientNetB4/EfficientNetB4-Covid-19.weights.h5\"\n",
    "model = keras.models.load_model(save)\n",
    "\n",
    "#model.load_weights(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2fsiEtEGG6-"
   },
   "source": [
    "#### **Generate CSV files containing classes indicies & image size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-05T22:42:20.149177Z",
     "iopub.status.busy": "2023-03-05T22:42:20.148486Z",
     "iopub.status.idle": "2023-03-05T22:42:20.163399Z",
     "shell.execute_reply": "2023-03-05T22:42:20.162351Z",
     "shell.execute_reply.started": "2023-03-05T22:42:20.149135Z"
    },
    "id": "UiHQzq8XGG6-",
    "outputId": "e2daeab5-c65c-495c-ffde-be259c917c07",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_dict = train_gen.class_indices\n",
    "img_size = train_gen.image_shape\n",
    "height = []\n",
    "width = []\n",
    "for _ in range(len(class_dict)):\n",
    "    height.append(img_size[0])\n",
    "    width.append(img_size[1])\n",
    "\n",
    "Index_series = pd.Series(list(class_dict.values()), name= 'class_index')\n",
    "Class_series = pd.Series(list(class_dict.keys()), name= 'class')\n",
    "Height_series = pd.Series(height, name= 'height')\n",
    "Width_series = pd.Series(width, name= 'width')\n",
    "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\n",
    "csv_name = f'{subject}-class_dict.csv'\n",
    "csv_save_loc = os.path.join(save_path, csv_name)\n",
    "class_df.to_csv(csv_save_loc, index= False)\n",
    "print(f'class csv file was saved as {csv_save_loc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GradCam from keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
    "\n",
    "# Load your fine-tuned EfficientNetB4 model\n",
    "#model = tf.keras.models.load_model(\"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/EfficientNetB4/EfficientNetB4-Covid-19-95.75.keras\")\n",
    "#model = tf.keras.models.load_model(\"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models/EfficientNetB4_masked/EfficientNetB4_masked-Covid-19_masked-71.09.keras\")\n",
    "# Extract the EfficientNetB4 model\n",
    "efficientnet = model.get_layer(\"efficientnetb4\")  # Replace \"sequential_1\" with the correct name\n",
    "#efficientnet = model.get_layer(\"dense_3\")\n",
    "\n",
    "# Helper function to process the image\n",
    "def process_image(img_path, target_size=(224, 224)):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Preprocess for EfficientNet\n",
    "    return img_array\n",
    "\n",
    "# Helper function to generate Grad-CAM heatmap\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Create a model that maps the input image to the activations of the last conv layer\n",
    "    # and the final predictions\n",
    "    grad_model = Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Get the gradient of the predicted class wrt the output feature map of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])  # Default: highest score\n",
    "        class_channel = preds[:, pred_index]\n",
    "    \n",
    "    # Gradient of the top predicted class with regard to the output feature map\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    \n",
    "    # Pool the gradients over all axes\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Weight the output feature map by the pooled gradients\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Helper function to overlay the heatmap on the image\n",
    "def overlay_heatmap(heatmap, img_path, alpha=0.4):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    img = np.array(img)\n",
    "\n",
    "    img = img[:, :, 0] # select only 1\n",
    "    \n",
    "    heatmap = np.uint8(255 * heatmap)  # Rescale heatmap to 0-255\n",
    "    heatmap = np.expand_dims(heatmap, axis=-1)\n",
    "    \n",
    "    # Resize heatmap to match image dimensions\n",
    "    heatmap = tf.image.resize(heatmap, (img.shape[0], img.shape[1])).numpy()\n",
    "    heatmap = tf.keras.preprocessing.image.array_to_img(heatmap)\n",
    "    heatmap = np.array(heatmap)\n",
    "    print(heatmap)\n",
    "    # Create a heatmap overlay\n",
    "    overlay = np.clip(img * (1 - alpha) + heatmap * alpha, 0, 255).astype(\"uint8\")\n",
    "    overlay = np.clip(img * (1 - alpha) + heatmap * alpha, 0, 255).astype(\"uint8\")\n",
    "    return overlay\n",
    "\n",
    "# Grad-CAM visualization\n",
    "def plot_gradcam(img_path, model, last_conv_layer_name, pred_index=None):\n",
    "    # Preprocess the image\n",
    "    img_array = process_image(img_path)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index)\n",
    "\n",
    "    plt.imshow(heatmap)\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    overlay = overlay_heatmap(heatmap, img_path)\n",
    "    \n",
    "    # Plot original image and heatmap\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Grad-CAM\")\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Parameters\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/raw/COVID-19_Radiography_Dataset/COVID/images/COVID-30.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/COVID/images/COVID-951.png_masked.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/COVID/images/COVID-250.png_masked.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/COVID/images/COVID-256.png_masked.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/COVID/images/COVID-256.png_masked.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/Normal/images/Normal-15.png_masked.png\"\n",
    "img_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_copy/Lung_Opacity/images/Lung_Opacity-120.png_masked.png\"\n",
    "\n",
    "first_layer = \"stem_conv\" #TO CHANGE\n",
    "middle_layer = \"block4f_expand_conv\" #TO CHANGE\n",
    "last_conv_layer_name = \"top_conv\"  # Last conv layer in EfficientNetB4\n",
    "\n",
    "a=\"\"\"\n",
    "last_conv_layer_name = \"dense_3\"\n",
    "# init \n",
    "inputs = tf.keras.Input(shape=(224, 224, 1))\n",
    "outputs = model(inputs)\n",
    "\"\"\"\n",
    "# Run Grad-CAM\n",
    "plot_gradcam(img_path, efficientnet, last_conv_layer_name) # model or efficientnet\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow_radio_pulmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
