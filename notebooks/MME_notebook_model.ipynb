{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Notebook de Model`\n",
    "\n",
    "Notebook de Mickael MELKOWSKI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Config`\n",
    "\n",
    "### `Import`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# check for GPU support\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# chemin relatif vers le dossier data\n",
    "home = pathlib.Path(\"../\")\n",
    "path_to_data = pathlib.Path(\"../data\")\n",
    "\n",
    "data_folder_path = path_to_data / \"raw\" / \"COVID-19_Radiography_Dataset\"\n",
    "output_path = path_to_data / \"processed\" / \"covid_19_masked_tiny_500\"\n",
    "folder_to_process = [\"Lung_Opacity\",\"COVID\",\"Normal\",\"Viral_Pneumonia\"]\n",
    "\n",
    "# model save path\n",
    "# \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/models\"\n",
    "model_save_path = home / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PyTorch`\n",
    "\n",
    "### `DataLoading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pathlib\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# config\n",
    "dataset_path = \"/home/tylio/code/Project_radio_pulmo/code/radio_pulmo/data/processed/covid_19_masked_tiny_500\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Normalization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les transformations (optionnel, mais recommandé)\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Redimensionne les images\n",
    "    transforms.ToTensor(),  # Convertit les images en tenseurs\n",
    "])\n",
    "\n",
    "# Charger les données à partir du dossier\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "#dataset_test = datasets.ImageFolder(root='Testing', transform=transform)\n",
    "\n",
    "# mean and std for normalization\n",
    "def get_mean_std(loader):\n",
    "    # Compute the mean and standard deviation of all pixels in the dataset\n",
    "    num_pixels = 0\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "        std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "mean, std = get_mean_std(loader)\n",
    "\n",
    "# Définir les transformations (optionnel, mais recommandé)\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Redimensionne les images\n",
    "    # transforms.RandomRotation(10), # Rotates the images upto Max of 10 Degrees\n",
    "    # transforms.RandomHorizontalFlip(p=0.4), #Performs Horizantal Flip over images \n",
    "    transforms.ToTensor(),  # Convertit les images en tenseurs\n",
    "    transforms.Normalize(mean=mean, std=std)  # normalize\n",
    "])\n",
    "\n",
    "# Re-Charger les données avec la nouvelle transformation\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Normalization and data augmentation`\n",
    "\n",
    "Test en ajoutant aussi un resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID', 'Lung_Opacity', 'Normal', 'Viral_Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "# Définir les transformations (optionnel, mais recommandé)\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Redimensionne les images\n",
    "    transforms.ToTensor(),  # Convertit les images en tenseurs\n",
    "])\n",
    "\n",
    "# Charger les données à partir du dossier\n",
    "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "#dataset_test = datasets.ImageFolder(root='Testing', transform=transform)\n",
    "\n",
    "# mean and std for normalization\n",
    "def get_mean_std(loader):\n",
    "    # Compute the mean and standard deviation of all pixels in the dataset\n",
    "    num_pixels = 0\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "        std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "loader = DataLoader(full_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "mean_nums, std_nums = get_mean_std(loader)\n",
    "\n",
    "# Définir les transformations (optionnel, mais recommandé)\n",
    "data_transforms = {\n",
    "    \"train\":transforms.Compose([\n",
    "        transforms.Resize((150,150)), #Resizes all images into same dimension\n",
    "        transforms.RandomRotation(10), # Rotates the images upto Max of 10 Degrees\n",
    "        transforms.RandomHorizontalFlip(p=0.4), #Performs Horizantal Flip over images \n",
    "        transforms.ToTensor(), # Coverts into Tensors\n",
    "        transforms.Normalize(mean = mean_nums, std=std_nums)]), # Normalizes\n",
    "\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((150,150)),\n",
    "        transforms.CenterCrop(150), #Performs Crop at Center and resizes it to 150x150\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean_nums, std = std_nums)\n",
    "    ])}\n",
    "\n",
    "# Re-Charger les données avec la nouvelle transformation\n",
    "def load_split_train_test(datadir, data_transforms, valid_size = .2):\n",
    "    train_data = datasets.ImageFolder(\n",
    "        datadir,\n",
    "        transform=data_transforms['train']\n",
    "        ) #Picks up Image Paths from its respective folders and label them\n",
    "    test_data = datasets.ImageFolder(\n",
    "        datadir,\n",
    "        transform=data_transforms['val']\n",
    "        )\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    dataset_size = {\"train\":len(train_idx), \"val\":len(test_idx)}\n",
    "    train_sampler = SubsetRandomSampler(train_idx) # Sampler for splitting train and val images\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=8) # DataLoader provides data from traininng and validation in batches\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=8)\n",
    "    return trainloader, testloader, dataset_size\n",
    "\n",
    "dataloader_train, dataloader_test, dataset_size = load_split_train_test(dataset_path, data_transforms, .2)\n",
    "dataloaders = {\"train\":dataloader_train, \"val\":dataloader_test}\n",
    "data_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
    "class_names = dataloader_train.dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Splitting data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset 80%, 20% --> [0.8, 0.2]\n",
    "dataset_train, dataset_test = torch.utils.data.random_split(full_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader object\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Baseline Model definition from hugging face`\n",
    "\n",
    "[Hugging face models image classification sort by downloads](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test ResNet-50\n",
    "\n",
    "[ResNet50_hugging_face](https://huggingface.co/microsoft/resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetForImageClassification(\n",
       "  (resnet): ResNetModel(\n",
       "    (embedder): ResNetEmbeddings(\n",
       "      (embedder): ResNetConvLayer(\n",
       "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (encoder): ResNetEncoder(\n",
       "      (stages): ModuleList(\n",
       "        (0): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (4): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (5): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "# import torchgen\n",
    "\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\"\"\"\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "# Adaptation to our 4 classes by changing the final layer\n",
    "num_labels = 4\n",
    "model.num_labels = num_labels\n",
    "# model.classifier = torch.nn.Linear(model.classifier.in_features, num_labels)\n",
    "model.classifier[-1] = torch.nn.Linear(model.classifier[-1].in_features, num_labels)\n",
    "\n",
    "device = \"cuda\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Baseline Model definition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "device = \"cuda\"\n",
    "model = nn.Sequential(\n",
    "   nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3), # output 16, 254, 254\n",
    "   nn.MaxPool2d(kernel_size=2), # output 16, 127, 127\n",
    "   nn.ReLU(),\n",
    "    \n",
    "   nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3), # 32, 125, 125\n",
    "   nn.ReLU(),\n",
    "   nn.MaxPool2d(kernel_size=2), # output 32, 62, 62\n",
    "    \n",
    "   nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3), # output  64, 60, 60\n",
    "   nn.ReLU(),\n",
    "   nn.MaxPool2d(kernel_size=2), # output 64, 30, 30\n",
    "    \n",
    "   nn.Flatten(),\n",
    "   nn.Linear(64 * 30 * 30, 64), # on précise ici la dim finale --> 64, 30, 30\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(64, 4)\n",
    "\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3,256,256), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Loss function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(dataloader_train))\n",
    "\n",
    "# Définir la fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "y_pred = model(X_batch.to(device))\n",
    "\n",
    "criterion(y_pred, y_batch.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Fitting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.notebook import tqdm\n",
    "epochs = 10\n",
    "\n",
    "# Définition de l'optimizer\n",
    "optimizer = optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Dans ce mode certaines couches du modèle agissent différemment\n",
    "    model.train()\n",
    "    loss_total = 0\n",
    "    # Barre de progression\n",
    "    progress_bar = tqdm(\n",
    "            dataloader_train, desc=\"Epoch {:1d}\".format(epoch), leave=True, disable=False\n",
    "        )\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        # Batch de données\n",
    "        X_batch, y_batch = batch\n",
    "        \n",
    "        # Device\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Gradient mis 0\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calcul de prédiction\n",
    "        y_pred = model(X_batch.to(torch.float32))\n",
    "\n",
    "        # Calcul de la fonction de perte\n",
    "        loss =  criterion(y_pred, y_batch) #torch.mean(torch.abs(y_pred- y_batch.to(torch.float32)))#\n",
    "        # Backpropagation : calculer le gradient de la loss en fonction de chaque couche\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipper le gradient entre 0 et 1 pour plus de stabilité\n",
    "        #  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Descente de gradient : actualisation des paramètres\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_total += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix(\n",
    "            {\n",
    "                \"training_loss\": \"{:.3f}\".format(loss_total/(i+1))}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    # Passer le modèle en évaluation\n",
    "    model.eval()\n",
    "    # Calculer la loss totale\n",
    "    loss_val_total = 0\n",
    "    # Stocker les prédictions et les vraies valeurs.\n",
    "    predictions, true_vals = [], []\n",
    "    for batch in dataloader_val:\n",
    "        X_batch, y_batch = batch\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Prédiction du modèle pour un batch donné\n",
    "            y_pred = model(X_batch.to(torch.float32))\n",
    "        # Calcul de la fonction de perte pour l'utiliser comme une métrique\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        # Cummuler la fonction de perte de tous les lots de données.\n",
    "        loss_val_total += loss.item()\n",
    "        # Enregistrer les prédictions pour les utiliser plus tard\n",
    "        predictions.extend(y_pred.detach().cpu().numpy())\n",
    "        # Enregistrer les vraies valeurs pour les utiliser plus tard\n",
    "        true_vals.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Loss du jeu de données val\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "    # Ensemble des prédictions du jeu de données\n",
    "    predictions = np.array(predictions)\n",
    "    # Id prediction\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    # Ensemble des vraies valeurs du jeu de données\n",
    "    true_vals = np.array(true_vals)\n",
    "    return {\"loss\":loss_val_avg, \"accuracy\":accuracy_score(true_vals, predictions)}\n",
    "\n",
    "\n",
    "metrics = evaluate(dataloader_test)\n",
    "\n",
    "print(f\"Loss: {metrics['loss']}\")\n",
    "print(f\"Accuracy : {metrics['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Confusion matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "on_cuda = True # computed using cuda\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in dataloader_test:\n",
    "    if on_cuda:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs) # Feed Network\n",
    "\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = folder_to_process\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], \n",
    "                     index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (6,4))\n",
    "sn.heatmap(df_cm, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DenseNet-121 architecture`\n",
    "\n",
    "Using tuto on kaggle: [covid-19-detection-pytorch-tutorial](https://www.kaggle.com/code/arunrk7/covid-19-detection-pytorch-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from random import shuffle\n",
    "\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Model creation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = folder_to_process\n",
    "device=\"cuda\"\n",
    "\n",
    "def CNN_Model(pretrained=True):\n",
    "    model = models.densenet121(pretrained=pretrained) # Returns Defined Densenet model with weights trained on ImageNet\n",
    "    num_ftrs = model.classifier.in_features # Get the number of features output from CNN layer\n",
    "    model.classifier = nn.Linear(num_ftrs, len(class_names)) # Overwrites the Classifier layer with custom defined layer for transfer learning\n",
    "    model = model.to(device) # Transfer the Model to GPU if available\n",
    "    return model\n",
    "\n",
    "model = CNN_Model(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Every model Training and evaluation`\n",
    "\n",
    "La partie ci-dessous contient le code pour le training et evaluation du model contenue dans la variable `model` avec le dataset d'entrainement de validation dans le dict `dataloaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Criterion & Optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy loss)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Specify optimizer which performs Gradient Descent\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) # Learning Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Training function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\":dataloader_train,\n",
    "               \"val\":dataloader_test}\n",
    "data_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
    "\n",
    "metrics = {\n",
    "    \"acc\":[],\n",
    "    \"val_acc\":[],\n",
    "    \"loss\":[],\n",
    "    \"val_loss\":[],\n",
    "}\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train() # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            current_loss = 0.0\n",
    "            current_corrects = 0\n",
    "            current_kappa = 0\n",
    "            val_kappa = list()\n",
    "\n",
    "            for inputs, labels in tqdm.tqdm(dataloaders[phase], desc=phase, leave=False):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # We need to zero the gradients in the Cache.\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Time to carry out the forward training poss\n",
    "                # We only need to log the loss stats if we are in training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # We want variables to hold the loss statistics\n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                current_corrects += torch.sum(preds == labels.data)\n",
    "                val_kappa.append(cohen_kappa_score(preds.cpu().numpy(), labels.data.cpu().numpy()))\n",
    "\n",
    "            epoch_loss = current_loss / data_sizes[phase]\n",
    "            epoch_acc = current_corrects.double() / data_sizes[phase]\n",
    "\n",
    "            if phase == 'val':\n",
    "                epoch_kappa = np.mean(val_kappa)\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} | {phase} Accuracy: {epoch_acc:.4f} | Kappa Score: {epoch_kappa:.4f}')\n",
    "                metrics[\"acc\"].append(float(epoch_acc))\n",
    "                metrics[\"loss\"].append(float(epoch_loss))\n",
    "            else:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} | {phase} Accuracy: {epoch_acc:.4f}')\n",
    "                metrics[\"val_acc\"].append(float(epoch_acc))\n",
    "                metrics[\"val_loss\"].append(float(epoch_loss))\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('Val loss Decreased from {:.4f} to {:.4f} \\nSaving Weights... '.format(best_loss, epoch_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_since = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_since // 60, time_since % 60))\n",
    "    print('Best val loss: {:.4f}'.format(best_loss))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Visual functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "class_names = folder_to_process\n",
    "\n",
    "#Statistics Based on ImageNet Data for Normalisation\n",
    "mean_nums = [mean, mean, mean]\n",
    "std_nums = [std, std, std]\n",
    "\n",
    "def imshow(inp, size =(30,30), title=None, normalized=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalized:\n",
    "        mean = mean_nums\n",
    "        std = std_nums\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title, size=30)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_handeled = 0\n",
    "    ax = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_handeled += 1    \n",
    "                ax = plt.subplot(num_images//2, 2, images_handeled)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('Actual: {} predicted: {}'.format(class_names[labels[j].item()],class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j], (5,5))\n",
    "\n",
    "                if images_handeled == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 10\n",
    "base_model = train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders, num_epochs=nb_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Saving`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path / \"pytorch/dense121_normalized.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Loading`\n",
    "\n",
    "See: [PyTorch saving_loading_models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(pretrained=True)\n",
    "model.load_state_dict(torch.load(model_save_path / \"pytorch/dense121_normalized.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Eval and visualisation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(base_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Accuracy and loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use metrics dict from train function\n",
    "\n",
    "nb_epochs = 10\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "# Courbe du score de test du réseau Dense 121\n",
    "plt.plot(np.arange(1 , nb_epochs + 1, 1),\n",
    "         metrics[\"acc\"], \n",
    "         label = 'acc Dense121',\n",
    "         color = 'blue')\n",
    "plt.plot(np.arange(1 , nb_epochs + 1, 1),\n",
    "         metrics[\"val_acc\"],\n",
    "         label = 'val_acc Dense121',\n",
    "         color = 'red')\n",
    "# Labels des axes\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "# Affichage de la légende\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# Courbe du score de test du réseau Dense 121\n",
    "plt.plot(np.arange(1 , nb_epochs + 1, 1),\n",
    "         metrics[\"loss\"], \n",
    "         label = 'loss Dense121',\n",
    "         color = 'blue')\n",
    "plt.plot(np.arange(1 , nb_epochs + 1, 1),\n",
    "         metrics[\"val_loss\"],\n",
    "         label = 'val_loss Dense121',\n",
    "         color = 'red')\n",
    "\n",
    "# Labels des axes\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "# Affichage de la légende\n",
    "plt.legend()\n",
    "# Affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Confusion Matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "on_cuda = True # computed using cuda\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in dataloader_test:\n",
    "    if on_cuda:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs) # Feed Network\n",
    "\n",
    "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "    y_pred.extend(output) # Save Prediction\n",
    "\n",
    "    labels = labels.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = folder_to_process\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(100*cf_matrix / np.sum(cf_matrix, axis=1)[:, None], \n",
    "                     index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (6,4))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('DenseNet121 Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Hugging Face model Training and evaluation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "Using [transformers/training tutorial](https://huggingface.co/docs/transformers/training)\n",
    "\n",
    "Create a [TrainingArguments](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments) class which contains all the hyperparameters you can tune as well as flags for activating different training options.\n",
    "\n",
    "With 'eval_strategy' parameter in your training arguments to report the evaluation metric at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer) does not automatically evaluate model performance during training. You’ll need to pass Trainer a function to compute and report metrics. The [🤗 Evaluate](https://huggingface.co/docs/evaluate/index) library provides a simple [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) function you can load with the evaluate.load (see this [quicktour](https://huggingface.co/docs/evaluate/a_quick_tour) for more information) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer object\n",
    "\n",
    "Create a [Trainer](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer) object with your model, training arguments, training and test datasets, and evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[1;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/accelerate/data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/trainer_utils.py:841\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    840\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py:131\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 131\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mvars\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    132\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/transformers/data/data_collator.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 131\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    132\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
