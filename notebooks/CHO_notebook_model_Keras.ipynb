{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des \n",
    "\n",
    "# Import des packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import gc\n",
    "#from scipy import sparse\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Keras et tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "path_to_data = r\"C:\\Users\\Inrae\\Documents\\Projet_Data_Science\"\n",
    "data_folder_path = os.path.join(path_to_data,\"COVID-19_Radiography_Dataset\")\n",
    "output_path = os.path.join(path_to_data,\"processed\")\n",
    "final_size=(224,224)\n",
    "\n",
    "# Import des données on sépare entre apprentissage et validation\n",
    "batch_size=16\n",
    "data_dir=output_path\n",
    "size=final_size\n",
    "\n",
    "div_dir = os.path.join(path_to_data,\"divided\")\n",
    "\n",
    "# Définir les chemins  # Répertoire original contenant les classes\n",
    "train_dir = os.path.join(div_dir,\"train\")\n",
    "val_dir =  os.path.join(div_dir,\"val\")\n",
    "test_dir =  os.path.join(div_dir,\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "# On vérifie que la gpu fonctionne\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En repartant du notebook de MME pour l'import des données\n",
    "## Preprocessing et export des données masquées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On importe les images préprocessées et masquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_data = r\"C:\\Users\\Inrae\\Documents\\Projet_Data_Science\"\n",
    "data_folder_path = os.path.join(path_to_data,\"COVID-19_Radiography_Dataset\")\n",
    "output_path = os.path.join(path_to_data,\"processed\")\n",
    "final_size=(224,224)\n",
    "\n",
    "# Import des données on sépare entre apprentissage et validation\n",
    "batch_size=16\n",
    "data_dir=output_path\n",
    "size=final_size\n",
    "\n",
    "# On laisse en couleur pour pouvoir utiliser les modeles preentrainés\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.5,\n",
    "    subset=\"training\",\n",
    "    #color_mode= \"grayscale\",\n",
    "    seed=42,\n",
    "    image_size=size,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.5,\n",
    "    subset=\"validation\",\n",
    "    #color_mode= \"grayscale\",\n",
    "    seed=42,\n",
    "    image_size=size,\n",
    "    batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création X_train, X_test, y_train, y_test\n",
    "\n",
    "# For train data\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in train_ds.take(-1):  # -1 takes all\n",
    "    all_images.append(images.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "X_train = np.concatenate(all_images)\n",
    "y_train = np.concatenate(all_labels)\n",
    "\n",
    "# For test data\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "for images, labels in val_ds.take(-1):  # -1 takes all\n",
    "    all_images.append(images.numpy())\n",
    "    all_labels.append(labels.numpy())\n",
    "\n",
    "X_test = np.concatenate(all_images)\n",
    "y_test = np.concatenate(all_labels)\n",
    "\n",
    "# normalization\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cree un generateur d image\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    #horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    #width_shift_range=0.1,\n",
    "    #height_shift_range=0.1  \n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Augmenter respectivement les jeu de données d'entrainement\n",
    "train_dataset = train_datagen.flow(X_train,y_train,   batch_size = 64)\n",
    "\n",
    "test_dataset = test_datagen.flow(X_test, y_test, batch_size = 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de DL\n",
    "## Test d un premier modèle from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction d un modèle classique\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Couche d'entrée pour les images 224x224 avec 3 canaux (RGB) \n",
    "model.add(Input(shape=(224, 224, 3))) \n",
    "\n",
    "# Première couche de convolution \n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same')) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "\n",
    "# Deuxième couche de convolution \n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) \n",
    "model.add(MaxPooling2D((2, 2))) \n",
    "\n",
    "# Troisième couche de convolution \n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Quatireme couche de convolution \n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "# Couche flatten pour transformer les cartes de caractéristiques en un vecteur \n",
    "model.add(Flatten()) \n",
    "\n",
    "# Ajouter des couches fully connected (denses) \n",
    "\n",
    "model.add(Dense(60, activation='relu')) \n",
    "model.add(Dense(4, activation='softmax')) \n",
    "\n",
    "#Compiler le modèle model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Définir le learning rate désiré \n",
    "learning_rate = 0.001 #\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "#Entrainement du modèle, utiliser le jeu de données augmenté, et préciser les callbacks \n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10 \n",
    "model_history = model.fit(\n",
    "            train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "            validation_data = test_dataset, # use augmented images for test\n",
    "            epochs = epochs,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET préentrainé sur image net\n",
    "### En utilisant directement le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe un modele préentrainé : REsnet50\n",
    "# https://keras.io/api/applications/#finetune-inceptionv3-on-a-new-set-of-classes\n",
    "#import h5py\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "\n",
    "#base_model.summary()\n",
    "\n",
    "\n",
    "len(base_model.layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sans data augmentation\n",
    "##### Avec un learning rate de 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del history\n",
    "del model\n",
    "del base_model\n",
    "del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avec un learning rate de 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "# del model\n",
    "# del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "learning_rate = 0.0001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =5,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avec un learning rate de 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "# del model\n",
    "# del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "learning_rate = 0.01 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReduceLRonPlateau et early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "del model\n",
    "del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "#learning_rate = 0.01 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    min_lr=0.0001,\n",
    "    min_delta =  0.05,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.01,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    lr_callback,\n",
    "    stop_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "#K.set_value(model.optimizer.lr, 1e-2)\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LR scheduler et early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "del model\n",
    "del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "#learning_rate = 0.01 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.01,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    scheduler_callback,\n",
    "    stop_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec data augmentation, LR scheduler et early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.005,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    scheduler_callback,\n",
    "    stop_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degel des paramètres des dernières couches\n",
    "#### On dégèle les 7 dernieres couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del history\n",
    "#del model\n",
    "#del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:170]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[170:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.005,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    scheduler_callback,\n",
    "    stop_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sauvegarde du model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degel des 15 dernieres couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "#del model\n",
    "#del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:160]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[160:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.005,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    scheduler_callback,\n",
    "    stop_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On redescebnd à un btach de 32 par manque de mémoire\n",
    "\n",
    "#del base_model\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On degele la moitie des couches et on teste un cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model\n",
    "    del model\n",
    "except NameError:\n",
    "    print(\"Error: model No value detected\")\n",
    "\n",
    "try:\n",
    "    base_model\n",
    "    del base_model\n",
    "except NameError:\n",
    "    print(\"Error: base_model No value detected\")\n",
    "\n",
    "try:\n",
    "    preprocess_input\n",
    "    del preprocess_input\n",
    "except NameError:\n",
    "    print(\"Error: preprocess_input No value detected\")\n",
    "\n",
    "\n",
    "try:\n",
    "    history\n",
    "    del history\n",
    "except NameError:\n",
    "    print(\"Error:history No value detected\")\n",
    "\n",
    "\n",
    "# try:\n",
    "#     train_datagen\n",
    "#     del train_datagen\n",
    "#     except NameError:\n",
    "#     print(\"Error:train_datagen No value detected\")\n",
    "\n",
    "\n",
    "# try:\n",
    "#     train_dataset\n",
    "#     del train_dataset\n",
    "#     except NameError:\n",
    "#     print(\"Error:train_dataset No value detected\")\n",
    "\n",
    "\n",
    "\n",
    "# Libérer de la mémoire\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "#del history\n",
    "#del model\n",
    "#del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=48\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = batch_size, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "# On prépare les poids pour le dataset\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               class_weight='balanced',\n",
    "               classes=np.unique(train_dataset.classes),\n",
    "               y=train_dataset.classes)\n",
    "\n",
    "class_weightDICT = dict(zip(np.unique(train_dataset.classes), class_weights))\n",
    "print(class_weightDICT)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top= False, pooling='max')\n",
    "x = base_model.output\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:100]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[100:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch) \n",
    "cosine_decay = CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "# def scheduler(epoch, lr):\n",
    "#     if epoch <= 2:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * np.exp(-0.05)\n",
    "    \n",
    "# scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# On cree un callback pour sauvegarder le meilleur modèle\n",
    "checkpoint_filepath = '../models/checkpoint/model_resnet50_CHO_v1'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.001,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    " #   scheduler_callback,\n",
    "    stop_callback,\n",
    "    model_checkpoint_callback,\n",
    "]\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay \n",
    "optimizer = Adam(learning_rate=cosine_decay)\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for \n",
    "            class_weight=class_weightDICT,\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#del base_model\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge le modèle\n",
    "\n",
    "#model = model.load('../models/model_resnet50_CHO_v1')\n",
    "model = tf.keras.models.load_model(\"../models/model_resnet50_CHO_v1\")\n",
    "\n",
    "#model.save('../models/model_resnet50_CHO_v1')\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "img_size = (299, 299)\n",
    "preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "decode_predictions = tf.keras.applications.resnet50.decode_predictions\n",
    "\n",
    "last_conv_layer_name = \"conv5_block3_3_conv\"\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # Modèle pour obtenir les gradients des activations de la dernière couche convolutive\n",
    "    grad_model = Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # Obtenir les gradients des activations de la dernière couche convolutive\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Multiplier chaque canal par l'importance de ce canal pour la classe prédite\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normaliser le heatmap entre 0 et 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Charger l'image\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    #img = cv2.imread(img_path) \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Redimensionner le heatmap pour qu'il corresponde à l'image\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = np.array(Image.fromarray(heatmap).resize((img.shape[1], img.shape[0])))\n",
    "\n",
    "    # Appliquer le heatmap sur l'image\n",
    "    heatmap = np.expand_dims(heatmap, axis=-1)\n",
    "    superimposed_img = alpha * img + heatmap  \n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    # # Afficher l'image seul\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1) \n",
    "    # Afficher le heatmap seul plt.subplot(1, 2, 1) \n",
    "    plt.imshow(heatmap[..., 0], cmap='viridis') \n",
    "    plt.title('Heatmap') \n",
    "    plt.axis('off') \n",
    "    \n",
    "    # Afficher l'image superposée \n",
    "    plt.subplot(1, 2, 2) \n",
    "    plt.imshow(superimposed_img, cmap='viridis') \n",
    "    plt.title('Superimposed Image') \n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "# Charger et prétraiter l'image\n",
    "img_path = os.path.join(data_dir,\"COVID\",\"COVID-30.png_masked.png\")\n",
    "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# Générer le heatmap Grad-CAM\n",
    "last_conv_layer_name = \"conv5_block3_out\"\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "# Superposer le heatmap sur l'image\n",
    "save_and_display_gradcam(img_path,heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Obtenir un batch d'images\n",
    "dataiter = iter(test_dataset)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "x = [random.randint(0, len(images)) for p in range(0, 6)]\n",
    "\n",
    "# # Parcourir les images du batch\n",
    "for i in x :\n",
    "     img_array = np.expand_dims(images[i], axis=0)\n",
    "     heatmap = make_gradcam_heatmap(img_array, model, \"conv5_block3_out\")\n",
    "     img_path = data_dir + '/' + test_dataset.filenames[i]\n",
    "     save_and_display_gradcam(img_path, heatmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# On met -0.5 et pas 0.5 pour ne pas avoir le fond en rouge\n",
    "\n",
    "def display_gradcam(img_path, heatmap, pred_label, true_label, alpha=-0.5):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    " # Afficher l'image superposée \n",
    "    plt.subplot(2, 1, 1) \n",
    "    plt.imshow(img, cmap='viridis') \n",
    "    plt.title(f'True: {true_label}') \n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 2) \n",
    "    plt.title(f' Pred: {pred_label}')\n",
    "    plt.imshow(superimposed_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Obtenir un batch d'images\n",
    "dataiter = iter(test_dataset)\n",
    "images, labels = next(dataiter)\n",
    "class_labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "\n",
    "x = [random.randint(0, (len(images)-1)) for p in range(0, 10)]\n",
    "\n",
    "\n",
    "# Parcourir les images du batch\n",
    "for i in x:\n",
    "    img_array = np.expand_dims(images[i], axis=0)\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, \"conv5_block3_out\")\n",
    "    \n",
    "    pred_label = class_labels[tf.argmax(model.predict(img_array)[0])]\n",
    "    true_label = class_labels[tf.argmax(labels[i])]\n",
    "\n",
    "    img_path = f\"{data_dir}/{test_dataset.filenames[i]}\"\n",
    "    display_gradcam(img_path, heatmap, pred_label, true_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model\n",
    "    del model\n",
    "except NameError:\n",
    "    print(\"Error: model No value detected\")\n",
    "\n",
    "try:\n",
    "    base_model\n",
    "    del base_model\n",
    "except NameError:\n",
    "    print(\"Error: base_model No value detected\")\n",
    "\n",
    "try:\n",
    "    preprocess_input\n",
    "    del preprocess_input\n",
    "except NameError:\n",
    "    print(\"Error: preprocess_input No value detected\")\n",
    "\n",
    "\n",
    "try:\n",
    "    history\n",
    "    del history\n",
    "except NameError:\n",
    "    print(\"Error:history No value detected\")\n",
    "\n",
    "\n",
    "# Libérer de la mémoire\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "#del history\n",
    "#del model\n",
    "#del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:150]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[150:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.001) \n",
    "\n",
    "# On crée des callback pour diminuer le LR\n",
    "\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch) \n",
    "cosine_decay = CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "# On cree un callback pour sauvegarder le meilleur modèle\n",
    "checkpoint_filepath = '../models/checkpoint/model_resnet50_CHO_v1'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    "    stop_callback,\n",
    "    model_checkpoint_callback,\n",
    "]\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay \n",
    "optimizer = Adam(learning_rate=cosine_decay)\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficientnet B2\n",
    "### Directement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour refaire de la place sur la gpu\n",
    "del model\n",
    "del cm\n",
    "import gc\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB2\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
    "\n",
    "base_model = EfficientNetB2(weights='imagenet')\n",
    "\n",
    "#base_model.summary()\n",
    "\n",
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del history\n",
    "#del model\n",
    "#del base_model\n",
    "#del cm\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "batch_size=64\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "  \n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "        validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model =  EfficientNetB2(weights='imagenet')\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs =10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace l evolution de la loss\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degel des dernieres couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# on degele les dix dernieres couches:\n",
    "for layer in model.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=0.1) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs = 20,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On trace l evolution de la loss\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait la matrice de confusion et le rapport de classification\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input, decode_predictions\n",
    "\n",
    "base_model = EfficientNetB0(weights='imagenet')\n",
    "\n",
    "#base_model.summary()\n",
    "\n",
    "\n",
    "len(base_model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# on degele les dix dernieres couches:\n",
    "for layer in model.layers:\n",
    "   layer.trainable = False\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# # train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs = 10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# on degele les dix dernieres couches:\n",
    "for layer in model.layers[:300]:\n",
    "   layer.trainable = False\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# # train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs = 10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42, shuffle=False)\n",
    "labels = list(test_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(test_dataset, test_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(test_dataset.classes, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(test_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les paramètres pour la séparation des données \n",
    "train_ratio = 0.7 # Pourcentage de données d'entraînement \n",
    "validation_ratio = 0.3 # Pourcentage de données de validation\n",
    "\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    " \n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    validation_split=validation_ratio,\n",
    "    ) \n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     validation_split=validation_ratio,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(data_dir,batch_size = 64, class_mode=\"categorical\", target_size=(224,224), subset='training', seed=42)\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(data_dir, batch_size = 64, class_mode= \"categorical\", target_size=(224,224), subset='validation', seed=42)\n",
    "\n",
    "learning_rate = 0.001 #\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# on degele l ensemble des couches:\n",
    "for layer in model.layers :\n",
    "   layer.trainable = True\n",
    "\n",
    "#Créer une instance de l'optimiseur Adam avec le learning rate personnalisé  \n",
    "optimizer = Adam(learning_rate=learning_rate) \n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# # train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for test\n",
    "            epochs = 10,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour simplifier les tests futurs : on divise directement le dataset en trois jeux de données distincts : \n",
    "## train, test, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Définir les chemins  # Répertoire original contenant les classes\n",
    "train_dir = os.path.join(div_dir,\"train\")\n",
    "val_dir =  os.path.join(div_dir,\"val\")\n",
    "test_dir =  os.path.join(div_dir,\"test\")\n",
    "\n",
    "# Créer les répertoires de destination s'ils n'existent pas\n",
    "for dir in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# Définir les ratios de partition\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Vérifier que les ratios totalisent 1\n",
    "assert (train_ratio + val_ratio + test_ratio) == 1.0, \"Les ratios doivent totaliser 1\"\n",
    "\n",
    "# Parcourir les classes dans le répertoire de données\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    \n",
    "    # Si ce n'est pas un dossier, on ignore\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    # Créer les répertoires de classe dans chaque ensemble\n",
    "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "    \n",
    "    # Obtenir la liste des images dans la classe\n",
    "    images = os.listdir(class_path)\n",
    "    images = [img for img in images if img.lower().endswith(('.png', '.jpg', '.jpeg'))]  # Filtrer les images\n",
    "\n",
    "    # Mélanger les images de manière aléatoire\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calculer les indices de séparation\n",
    "    total_images = len(images)\n",
    "    train_end = int(train_ratio * total_images)\n",
    "    val_end = int((train_ratio + val_ratio) * total_images)\n",
    "\n",
    "    # Diviser les images en 3 ensembles\n",
    "    #total_images = len(images)\n",
    "    train_images = images[:train_end]\n",
    "    val_images = images[train_end:val_end]\n",
    "    test_images = images[val_end:]\n",
    "\n",
    "    #print((train_images + val_images + test_images), total_images, train_images, val_images, test_images)\n",
    "    # Vérifier que le total est ok\n",
    "    assert (len(train_images) + len(val_images) + len(test_images) == total_images), \"Le nombre total d images doit correspondre\"\n",
    "\n",
    "    # Déplacer les fichiers dans les répertoires appropriés\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))\n",
    "\n",
    "    for img in val_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))\n",
    "\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(test_dir, class_name, img))\n",
    "\n",
    "    print(f\"Class {class_name} partitionnée : {len(train_images)} train, {len(val_images)} val, {len(test_images)} test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNET50 avec CosineDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del base_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size=8\n",
    "# On reprocesse les jeux de données d'entrainement et de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    ) \n",
    "\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(train_dir,batch_size = batch_size, class_mode=\"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_val_datagen.flow_from_directory(test_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "val_dataset = test_val_datagen.flow_from_directory(val_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=False)\n",
    "\n",
    "# On prépare les poids pour le dataset\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               class_weight='balanced',\n",
    "               classes=np.unique(train_dataset.classes),\n",
    "               y=train_dataset.classes)\n",
    "\n",
    "class_weightDICT = dict(zip(np.unique(train_dataset.classes), class_weights))\n",
    "print(class_weightDICT)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top= False, pooling='max')\n",
    "x = base_model.output\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:100]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[100:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch) \n",
    "cosine_decay = CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "\n",
    "# On cree un callback pour sauvegarder le meilleur modèle\n",
    "checkpoint_filepath = '../models/checkpoint/model_resnet50_CHO'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    mode=\"min\", \n",
    "    min_delta =  0.001,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    " #   scheduler_callback,\n",
    " #   stop_callback,\n",
    "    model_checkpoint_callback,\n",
    "]\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay \n",
    "optimizer = Adam(learning_rate=cosine_decay)\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for \n",
    "            class_weight=class_weightDICT,\n",
    "            epochs =10,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB4 avec cosine decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del base_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    ) \n",
    "\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(train_dir,batch_size = batch_size, class_mode=\"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_val_datagen.flow_from_directory(test_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "val_dataset = test_val_datagen.flow_from_directory(val_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=False)\n",
    "\n",
    "\n",
    "# On prépare les poids pour le dataset\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               class_weight='balanced',\n",
    "               classes=np.unique(train_dataset.classes),\n",
    "               y=train_dataset.classes)\n",
    "\n",
    "class_weightDICT = dict(zip(np.unique(train_dataset.classes), class_weights))\n",
    "print(class_weightDICT)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = EfficientNetB4(weights='imagenet', include_top= False, pooling='max')\n",
    "x = base_model.output\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:100]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[100:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch) \n",
    "cosine_decay = CosineDecay(initial_learning_rate, decay_steps)\n",
    "\n",
    "# On cree un callback pour sauvegarder le meilleur modèle\n",
    "checkpoint_filepath = '../models/checkpoint/model_efficientnetB4_CHO_cos'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=2, \n",
    "    mode=\"max\", \n",
    "    min_delta =  0.001,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    " #   scheduler_callback,\n",
    "    stop_callback,\n",
    "    model_checkpoint_callback,\n",
    "]\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay \n",
    "optimizer = Adam(learning_rate=cosine_decay)\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for \n",
    "            class_weight=class_weightDICT,\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = list(val_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(val_dataset, val_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(val_dataset.classes, y_pred, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(val_dataset.classes, y_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/model_CHO_efficientnetB4_cos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB4 avec expdecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del history\n",
    "del model\n",
    "del base_model\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "# Pas ideal car on applique un image generator sur le jeu de validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range = 0.1, # random application of shearing\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    brightness_range = (0.4, 0.6),\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    "    ) \n",
    "\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "     preprocessing_function=preprocess_input,\n",
    "     ) \n",
    "\n",
    "# Est-on sur dans ce cas que la validation est bien indépendante de l'apprentissage ?\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(train_dir,batch_size = batch_size, class_mode=\"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "test_dataset = test_val_datagen.flow_from_directory(test_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=True)\n",
    "\n",
    "val_dataset = test_val_datagen.flow_from_directory(val_dir, batch_size = batch_size, class_mode= \"categorical\", target_size=(224,224), seed=42, shuffle=False)\n",
    "\n",
    "\n",
    "# On prépare les poids pour le dataset\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "               class_weight='balanced',\n",
    "               classes=np.unique(train_dataset.classes),\n",
    "               y=train_dataset.classes)\n",
    "\n",
    "class_weightDICT = dict(zip(np.unique(train_dataset.classes), class_weights))\n",
    "print(class_weightDICT)\n",
    "\n",
    "# On recrée le modèle de zéro car sinon les poids s ajustent au fur et à mesure\n",
    "\n",
    "base_model = EfficientNetB4(weights='imagenet', include_top= False, pooling='max')\n",
    "x = base_model.output\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(60, activation='relu')(x) \n",
    "predictions = Dense(4, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# first:  freeze all convolutional InceptionV3 layers\n",
    "\n",
    "for layer in model.layers[:100]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[100:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "# Définir le scheduler CosineDecay \n",
    "initial_learning_rate = 0.001 \n",
    "decay_steps = (train_dataset.n // batch_size) * 20 # Nombre total d'étapes (epochs * steps_per_epoch) \n",
    "decay_rate = 0.96\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "# Créer l'optimiseur Adam avec CosineDecay\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# On cree un callback pour sauvegarder le meilleur modèle\n",
    "checkpoint_filepath = '../models/checkpoint/model_efficientnetB4_CHO_exp'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=2, \n",
    "    mode=\"max\", \n",
    "    min_delta =  0.001,\n",
    "    verbose = 1 )\n",
    "\n",
    "my_callbacks = [\n",
    " #   scheduler_callback,\n",
    "#    stop_callback,\n",
    "    model_checkpoint_callback,\n",
    "]\n",
    "\n",
    "# Compiler le modèle avec l'optimiseur personnalisé \n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "history = model.fit(train_dataset, # use augmented images for train \n",
    "            steps_per_epoch=train_dataset.n // batch_size,\n",
    "            validation_data = test_dataset, # use initial images for \n",
    "            class_weight=class_weightDICT,\n",
    "            epochs =20,\n",
    "            verbose=True,\n",
    "            callbacks=[my_callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model acc by epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = list(val_dataset.class_indices.keys())\n",
    "\n",
    "Y_pred = model.predict(val_dataset, val_dataset.n // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(val_dataset.classes, y_pred, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels= labels)\n",
    "disp.plot(cmap='Blues')\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(val_dataset.classes, y_pred, target_names=labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
